{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "F9ERgBpbcMmB"
      },
      "source": [
        "# HW1: Frame-Level Speech Recognition"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CLkH6GMGcWcE"
      },
      "source": [
        "In this homework, you will be working with MFCC data consisting of 27 features at each time step/frame. Your model should be able to recognize the phoneme occured in that frame."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "4pL_44hWVwnF"
      },
      "source": [
        "#Instruction to Run the Code"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VOlMXjoWV3T_"
      },
      "source": [
        "# To run the final model corresponding to the highest kaggle submission, please first make sure the **Global Variables** (next section) are set to fit the purpose, and then go to **Runtime**, click **Restart and run all**. This would\n",
        "1. pip install, import and download all required packages and data\n",
        "2. run all functions and classes for loading data and creating model/optimizer/scheduler\n",
        "3. train the model based on the parameters saved in the config_king variable defined under **Parameter Configuration**.\n",
        "\n",
        "**Note** that by default, the training data loads the combined 360 and 100 data. If you only want to run with 360 or 100, please set **ONLY360** or **ONLY100** as True under Global Variable section. If both are set to True, train 360 would be preferred.\n",
        "\n",
        "**Note** that to run the combined 460 train data, the notebook uses GCP which allow 102.5 GB RAM.\n",
        "\n",
        "By default, the notebook would run the trained model on the test dataset and save the predicted result in csv file, but it would not make the submission to Kaggle. To run the notebook with kaggle submission, set **SUBMIT_KAGGLE** to True."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wi-KMr2iW_Jf"
      },
      "source": [
        "# Global Variables"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h-wmAyy3XBH9"
      },
      "outputs": [],
      "source": [
        "ONLY360 = False\n",
        "ONLY100 = False\n",
        "CONNECT_DRIVE = False\n",
        "SUBMIT_KAGGLE = False\n",
        "MODEL_SAVED_NAME = \"460Attempt.pt\" # please change the name based on run schemes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YIugQjy-_Ax7"
      },
      "source": [
        "# README"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "--PrbtBgCxTW"
      },
      "source": [
        "## Best Score Hyperparameters:\n",
        "* **context** = 30\n",
        "* **init learning rate** = 1e-3\n",
        "* **layers** = 2048, 2048, 2048, 2048, 1280, 1024\n",
        "* **activation** = SiLU\n",
        "* **dropout** = 0.35 (on 2048 layer), 0.15 (on 1024 layer)\n",
        "* **weight decay** = 0.005\n",
        "* **batch size** = 16384\n",
        "* **epoch** = 40\n",
        "* **batchnorm**: on every layer\n",
        "* **weight init**: kaiming normal on linear layer, constant on batchnorm layer\n",
        "* **scheduler**: ReduceLROnPlateau\n",
        "** patience = 3\n",
        "** factor = 0.5\n",
        "** threshold = 0.0025\n",
        "** mode = max\n",
        "* **optimizer**: AdamW\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Data Loading Scheme:\n",
        "The highest kaggle score is run by loading train 360 and train 100, combining the two datasets into one, and training on the combined dataset. Note that the mfcc tensors are using type = float32 to save the memory. The detailed code is shown in **AudioDatasetBoth**, where mfcc from 360 and 100 are loaded individually in the order of 360 >> 100, saved into a list before final concatenation.\n",
        "\n",
        "\n",
        "The normal data loader that loads either 360 or 100 or validation are written in class **AudioDataset**, where all mfcc files are read in order and saved into a list for the final concatenation. No memory handling is used.\n",
        "\n",
        "\n",
        "## Architectures:\n",
        "The highest kaggle score is reached using **6 layer Pyramid** architecture: **2048-2048-2048-2048-1280-1042**. 1280 comes from 1.25 x 1024, and the purpose is to make the best use of the 20M allowed parameters. The model results in 19.9M parameters, which is within the limitation. Each linear layer (except for the output layer) is followed by a 1d batchnorm layer, and then SiLU activation. All layers with 2048 nodes have a dropout layer after activation with dropout rate at 0.35. For the last layer before the output layer, a dropout layer is added with 0.15 dropout rate.\n",
        "\n",
        "\n",
        "Other architectures are tested as well, but with less ideal performance:\n",
        "1. 8 layer Pyramid architecture, 2048 x 2 + 1024 x 6. Each layer has 1d batchnorm and uses SiLU activation, and for each alternative layer, a dropout layer with rate = 0.35 is added after activation.\n",
        "2. 8 layer Pyramid architecture, 2048 x 2 + 1024 x 4 + 512 x 2. Everything else is the same.\n",
        "3. 8 layer cylinder architecture, 1024 x 8. Everything else the same.\n",
        "4. 6 layer Pyramid architecture, 2048 x 4 + 1024 x 2. Batchnorm and SiLU are the same as above, and 5 out of 6 layers have the dropout layer. For a layer with 2048 nodes, the dropout rate is 0.35. For the last layer before output, the dropout rate is 0.15.\n",
        "5. 5 layer cylinder architecture, 2048 x 5. Batchnorm and SiLU are the same as above, and every layer has a dropout layer using 0.35 rate.\n",
        "\n",
        "\n",
        "## Epochs\n",
        "I trained the model for 40 epochs to get to my highest kaggle submission. The model is close to converging at 40 epoch, and I believe the model could hit the high-cutoff with 10 more epoch. However, due to time and computation unit restriction, I halted at 40 epochs.\n",
        "\n",
        "\n",
        "## Hyperparameters\n",
        "* **Context Size**: 30. I increased from 20 to 25 to 30, and found 30 the best performance.\n",
        "\n",
        "* **Batch Size**: 16384. On train 100, I use 1024 for various ablation tests. Once I switched to 360, to improve the runtime, I tested 4096 and 8192, and the accuracy is very satisfying using 8192. I then decided to move to the larger combined 460 training set, and I doubled the batch size to 16384.\n",
        "\n",
        "* **Weight Decay**: 0.005. As shown in my ablation study in the toy dataset (see excel), using 0.005 gives a better performance. But for the first 5 epochs, it is not as good as not using weight decay at all. When I try the weight decay on train 360, however, adding weight decay gives better performance in later epoch (10+) than not adding, so I keep the 0.005 weight decay in my final optimizer setting.\n",
        "\n",
        "* **Dropout**: 0.35 on layers with 2048 nodes, and 0.15 on the last layer with 1024 nodes before the output layer. For dropout, I tested 0.25, 0.3 and 0.35 on different layers, and using 0.35 on layers with 2048 nodes gives the better result. However, if applying 0.35 dropout on all 6 layers, the model tends to be very underfit. I then decreased the dropout in the last layer with least nodes, and removed the dropout layer from the second last layer, to get the best performance. However, overall, the model is still underfit (with train acc around 2% below val acc).\n",
        "\n",
        "\n",
        "\n",
        "\n",
        "## Other Experiments\n",
        "### Activation Function\n",
        "I selected the SiLU activation function (it seems to work better with batchnorm, theoretically), but I also tested LeakyRelu and GELU. In addition, I tried using different activation functions on different layers, and the result is not as good as simply using SiLU.\n",
        "## Batchnorm\n",
        "I used batchnorm on every layer, but tested alternative batchnorm as well. Using it on every layer performs better.\n",
        "### Weight Initialization\n",
        "I used normal Kaiming initialization for linear layer weight, and constant 1 vs 0 (weight vs bias) initialization for 1d batchnorm layer. I also tried Kaiming uniform initialization, and the performance is slightly worse.\n",
        "### Scheduler and Learning Rate\n",
        "I selected ReduceLROnPlateau with patience 3 and factor 0.5, with threshold 0.0025 and max mode. I started with using patience = 2 and threshold = 0.01, but the learning rate started to decrease before the model converges. I then used the default threshold 1e-4 and it took a lot of epochs to train before the learning rate decreased (~50). Hence I found the middle point, used the 0.0025 threshold and increased the patience by 1. The learning rate started to decrease at around 18 epoch by half. I used max mode aiming to adjust learning rate based on val accuracy. Overall, the scheduler helps improve the val accuracy, but since I used a large threshold, the learning rate still decreases before the original model using initial learning rate fully converges. This could be the reason for my model to not yet reach the high cutoff at epoch 40.\n",
        "\n",
        "\n",
        "### Optimizer\n",
        "I only switched between AdamW and Adam optimizer, and found the former better.\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "z4vZbDmJvMp1"
      },
      "source": [
        "# Libraries"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rwYu9sSUnSho",
        "outputId": "2b887292-c61c-43a1-b884-5f5c8c80e61d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m2.0/2.0 MB\u001b[0m \u001b[31m22.8 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m184.3/184.3 KB\u001b[0m \u001b[31m20.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m181.3/181.3 KB\u001b[0m \u001b[31m21.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m62.7/62.7 KB\u001b[0m \u001b[31m7.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m140.6/140.6 KB\u001b[0m \u001b[31m17.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Building wheel for pathtools (setup.py) ... \u001b[?25l\u001b[?25hdone\n"
          ]
        }
      ],
      "source": [
        "!pip install torchsummaryX wandb --quiet"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "qI4qfx7tiBZt",
        "outputId": "e1c2bb67-20a1-4eaf-c0b0-600d917cec5c"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Device:  cuda\n"
          ]
        }
      ],
      "source": [
        "import torch\n",
        "import numpy as np\n",
        "from torchsummaryX import summary\n",
        "import sklearn\n",
        "import gc\n",
        "import zipfile\n",
        "import pandas as pd\n",
        "from tqdm.auto import tqdm\n",
        "import os\n",
        "import datetime\n",
        "import wandb\n",
        "device = 'cuda' if torch.cuda.is_available() else 'cpu'\n",
        "print(\"Device: \", device)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8yBgXjKV1O0Z"
      },
      "outputs": [],
      "source": [
        "### If you are using colab, you can import google drive to save model checkpoints in a folder\n",
        "if CONNECT_DRIVE:\n",
        "  from google.colab import drive\n",
        "  drive.mount('/content/gdrive')\n",
        "  # !ls /content/drive/MyDrive"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N-9qE20hmCgQ"
      },
      "outputs": [],
      "source": [
        "### PHONEME LIST\n",
        "PHONEMES = [\n",
        "            '[SIL]',   'AA',    'AE',    'AH',    'AO',    'AW',    'AY',\n",
        "            'B',     'CH',    'D',     'DH',    'EH',    'ER',    'EY',\n",
        "            'F',     'G',     'HH',    'IH',    'IY',    'JH',    'K',\n",
        "            'L',     'M',     'N',     'NG',    'OW',    'OY',    'P',\n",
        "            'R',     'S',     'SH',    'T',     'TH',    'UH',    'UW',\n",
        "            'V',     'W',     'Y',     'Z',     'ZH',    '[SOS]', '[EOS]']"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ZIi0Big7vPa9"
      },
      "source": [
        "# Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BBCbeRhixGM7"
      },
      "source": [
        "This section contains code that helps you install kaggle's API, creating kaggle.json with you username and API key details. Make sure to input those in the given code to ensure you can download data from the competition successfully."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "TPBUd7Cnl-Rx",
        "outputId": "36f9a775-a7b7-4661-d23c-5b3c9ad083c5"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Looking in indexes: https://pypi.org/simple, https://us-python.pkg.dev/colab-wheels/public/simple/\n",
            "Collecting kaggle==1.5.8\n",
            "  Downloading kaggle-1.5.8.tar.gz (59 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m59.2/59.2 KB\u001b[0m \u001b[31m2.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25h  Preparing metadata (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "Building wheels for collected packages: kaggle\n",
            "  Building wheel for kaggle (setup.py) ... \u001b[?25l\u001b[?25hdone\n",
            "  Created wheel for kaggle: filename=kaggle-1.5.8-py3-none-any.whl size=73274 sha256=7f783382b6f0d1420350aa39431cc6dcbd6943c67b657bc47d6dfecdfb59ed20\n",
            "  Stored in directory: /root/.cache/pip/wheels/f3/67/7b/a6d668747974998471d29b230e7221dd01330ac34faebe4af4\n",
            "Successfully built kaggle\n",
            "Installing collected packages: kaggle\n",
            "  Attempting uninstall: kaggle\n",
            "    Found existing installation: kaggle 1.5.12\n",
            "    Uninstalling kaggle-1.5.12:\n",
            "      Successfully uninstalled kaggle-1.5.12\n",
            "Successfully installed kaggle-1.5.8\n"
          ]
        }
      ],
      "source": [
        "!pip install --upgrade --force-reinstall --no-deps kaggle==1.5.8\n",
        "!mkdir /root/.kaggle\n",
        "\n",
        "with open(\"/root/.kaggle/kaggle.json\", \"w+\") as f:\n",
        "    f.write('{\"username\":\"sharonxin1207\",\"key\":\"xxx\"}')\n",
        "    # Put your kaggle username & key here\n",
        "\n",
        "!chmod 600 /root/.kaggle/kaggle.json"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "if2Somqfbje1",
        "outputId": "61d609f7-592a-4b5c-f092-d9a8eb1b834b"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Downloading 11-785-s23-hw1p2.zip to /content\n",
            "100% 16.0G/16.0G [12:33<00:00, 29.9MB/s]\n",
            "100% 16.0G/16.0G [12:33<00:00, 22.8MB/s]\n"
          ]
        }
      ],
      "source": [
        "# commands to download data from kaggle\n",
        "\n",
        "!kaggle competitions download -c 11-785-s23-hw1p2\n",
        "!mkdir '/content/data'\n",
        "\n",
        "!unzip -qo '11-785-s23-hw1p2.zip' -d '/content/data'\n",
        "#!cat ../root/.kaggle/kaggle.json\n",
        "#!cp -r \"/content/data\" '/content/drive/MyDrive'"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "VmobrpsjMlk3"
      },
      "source": [
        "For QUIZ MCQ Exploration"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "807TvJs2X2rW"
      },
      "outputs": [],
      "source": [
        "from collections import Counter\n",
        "def transcript_explore():\n",
        "  # Code for MCQ QUIZ\n",
        "  transcript_dir = \"/content/drive/MyDrive/DLdata/11-785-s23-hw1p2/dev-clean/transcript\"\n",
        "  transcript_names  = sorted(os.listdir(transcript_dir))\n",
        "  transcript_list = []\n",
        "  for i in range(len(transcript_names)):\n",
        "  #  print(transcript_names[i])\n",
        "    try:\n",
        "      transcript  = np.load(transcript_dir+\"/\"+transcript_names[i])\n",
        "      transcript_list.append(transcript)\n",
        "      transcripts = np.concatenate(transcript_list)\n",
        "    except ValueError:\n",
        "      print(transcript_names[i])\n",
        "\n",
        "  sil_count = np.count_nonzero(transcripts == '[SIL]')\n",
        "  counter = Counter(transcripts)\n",
        "  print(sil_count)\n",
        "  print(counter.most_common()[-1][0])\n",
        "\n",
        "# transcript_explore()"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Vuzce0_TdcaR"
      },
      "source": [
        "# Dataset"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2_7QgMbBdgPp"
      },
      "source": [
        "This section covers the dataset/dataloader class for speech data. You will have to spend time writing code to create this class successfully. We have given you a lot of comments guiding you on what code to write at each stage, from top to bottom of the class. Please try and take your time figuring this out, as it will immensely help in creating dataset/dataloader classes for future homeworks.\n",
        "\n",
        "Before running the following cells, please take some time to analyse the structure of data. Try loading a single MFCC and its transcipt, print out the shapes and print out the values. Do the transcripts look like phonemes?"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "YpLCvi3AJC5z"
      },
      "outputs": [],
      "source": [
        "# Dataset class to load train and validation data\n",
        "class AudioDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, phonemes = PHONEMES, context=0, partition= \"train-clean-100\"): # Feel free to add more arguments\n",
        "\n",
        "        self.context    = context\n",
        "        self.phonemes   = phonemes\n",
        "\n",
        "        # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data using root\n",
        "        self.mfcc_dir       = \"{}/{}/mfcc\".format(root, partition)\n",
        "        # TODO: Transcripts directory - use partition to acces train/dev directories from kaggle data using root\n",
        "        self.transcript_dir = \"{}/{}/transcript\".format(root, partition)\n",
        "\n",
        "\n",
        "        # TODO: List files in sefl.mfcc_dir using os.listdir in sorted order\n",
        "        mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n",
        "        # TODO: List files in self.transcript_dir using os.listdir in sorted order\n",
        "        transcript_names    = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        # Making sure that we have the same no. of mfcc and transcripts\n",
        "        assert len(mfcc_names) == len(transcript_names)\n",
        "\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        # TODO: Iterate through mfccs and transcripts\n",
        "        for i in range(len(mfcc_names)):\n",
        "        #   Load a single mfcc\n",
        "            mfcc        = np.load(self.mfcc_dir+\"/\"+mfcc_names[i])\n",
        "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "            mfcc        = (mfcc - mfcc.mean(axis=0))/mfcc.std(axis=0)\n",
        "        #   Load the corresponding transcript\n",
        "            transcript  = np.load(self.transcript_dir+\"/\"+transcript_names[i])\n",
        "            # Remove [SOS] and [EOS] from the transcript\n",
        "            # (Is there an efficient way to do this without traversing through the transcript?)\n",
        "            # Note that SOS will always be in the starting and EOS at end, as the name suggests.\n",
        "            start_idx = np.where(transcript=='[SOS]')[0][-1]\n",
        "            end_idx = np.where(transcript=='[EOS]')[0][0]\n",
        "            transcript = transcript[start_idx+1:end_idx]\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)\n",
        "\n",
        "        # TODO: Concatenate all mfccs in self.mfccs such that\n",
        "        # the final shape is T x 27 (Where T = T1 + T2 + ...)\n",
        "        self.mfccs          = np.concatenate(self.mfccs)\n",
        "        #self.mfccs.astype(np.float32)\n",
        "\n",
        "        # TODO: Concatenate all transcripts in self.transcripts such that\n",
        "        # the final shape is (T,) meaning, each time step has one phoneme output\n",
        "        self.transcripts    = np.concatenate(self.transcripts)\n",
        "        #self.transcripts.astype(np.uint8)\n",
        "        # Hint: Use numpy to concatenate\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "        # Take some time to think about what we have done.\n",
        "        # self.mfcc is an array of the format (Frames x Features).\n",
        "        # Our goal is to recognize phonemes of each frame\n",
        "        # From hw0, you will be knowing what context is.\n",
        "        # We can introduce context by padding zeros on top and bottom of self.mfcc\n",
        "        self.mfccs = np.pad(self.mfccs, ((self.context, self.context), (0, 0)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "        # The available phonemes in the transcript are of string data type\n",
        "        # But the neural network cannot predict strings as such.\n",
        "        # Hence, we map these phonemes to integers\n",
        "\n",
        "        # TODO: Map the phonemes to their corresponding list indexes in self.phonemes\n",
        "        self.transcripts = np.array([phonemes.index(p) for p in self.transcripts]).reshape(self.transcripts.shape)\n",
        "        # Now, if an element in self.transcript is 0, it means that it is 'SIL' (as per the above example)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
        "        frames = self.mfccs[ind:(ind+2*self.context+1)]\n",
        "        # After slicing, you get an array of shape 2*context+1 x 27. But our MLP needs 1d data and not 2d.\n",
        "        frames = frames.flatten() # TODO: Flatten to get 1d data\n",
        "\n",
        "        frames      = torch.FloatTensor(frames) # Convert to tensors\n",
        "        phonemes    = torch.tensor(self.transcripts[ind])\n",
        "\n",
        "        return frames, phonemes"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "e8KfVP39S6o7"
      },
      "outputs": [],
      "source": [
        "class AudioTestDataset(torch.utils.data.Dataset):\n",
        "\n",
        "    # TODO: Create a test dataset class similar to the previous class but you dont have transcripts for this\n",
        "    # Imp: Read the mfccs in sorted order, do NOT shuffle the data here or in your dataloader.\n",
        "    def __init__(self, root, phonemes = PHONEMES, context=0, partition= \"test-clean-100\"): # Feel free to add more arguments\n",
        "\n",
        "        self.context    = context\n",
        "        self.phonemes   = phonemes\n",
        "\n",
        "        # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data using root\n",
        "        self.mfcc_dir       = \"{}/{}/mfcc\".format(root, partition)\n",
        "\n",
        "        # TODO: List files in sefl.mfcc_dir using os.listdir in sorted order\n",
        "        mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n",
        "\n",
        "        self.mfccs = []\n",
        "\n",
        "        # TODO: Iterate through mfccs and transcripts\n",
        "        for i in range(len(mfcc_names)):\n",
        "        #   Load a single mfcc\n",
        "            mfcc        = np.load(self.mfcc_dir+\"/\"+mfcc_names[i])\n",
        "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "\n",
        "            mfcc        = (mfcc - mfcc.mean(axis=0))/mfcc.std(axis=0)\n",
        "            self.mfccs.append(mfcc)\n",
        "\n",
        "        # NOTE:\n",
        "        # Each mfcc is of shape T1 x 27, T2 x 27, ...\n",
        "        # Each transcript is of shape (T1+2) x 27, (T2+2) x 27 before removing [SOS] and [EOS]\n",
        "\n",
        "        # TODO: Concatenate all mfccs in self.mfccs such that\n",
        "        # the final shape is T x 27 (Where T = T1 + T2 + ...)\n",
        "        self.mfccs          = np.concatenate(self.mfccs)\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "        # Take some time to think about what we have done.\n",
        "        # self.mfcc is an array of the format (Frames x Features).\n",
        "        # Our goal is to recognize phonemes of each frame\n",
        "        # From hw0, you will be knowing what context is.\n",
        "        # We can introduce context by padding zeros on top and bottom of self.mfcc\n",
        "        self.mfccs = np.pad(self.mfccs, ((self.context, self.context), (0, 0)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "        # The available phonemes in the transcript are of string data type\n",
        "        # But the neural network cannot predict strings as such.\n",
        "        # Hence, we map these phonemes to integers\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
        "        frames = self.mfccs[ind:(ind+2*self.context+1)]\n",
        "        # After slicing, you get an array of shape 2*context+1 x 27. But our MLP needs 1d data and not 2d.\n",
        "        frames = frames.flatten() # TODO: Flatten to get 1d data\n",
        "\n",
        "        frames      = torch.FloatTensor(frames) # Convert to tensors\n",
        "\n",
        "        return frames"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "IMn4OdnsyM9F"
      },
      "outputs": [],
      "source": [
        "# Dataset class to load train 100 and 360 data\n",
        "class AudioDatasetBoth(torch.utils.data.Dataset):\n",
        "\n",
        "    def __init__(self, root, phonemes = PHONEMES, context=0): # Feel free to add more arguments\n",
        "\n",
        "        self.context    = context\n",
        "        self.phonemes   = phonemes\n",
        "\n",
        "        # TODO: MFCC directory - use partition to acces train/dev directories from kaggle data using root\n",
        "        self.mfcc_dir       = \"{}/{}/mfcc\".format(root, \"train-clean-360\")\n",
        "        # TODO: Transcripts directory - use partition to acces train/dev directories from kaggle data using root\n",
        "        self.transcript_dir = \"{}/{}/transcript\".format(root, \"train-clean-360\")\n",
        "\n",
        "        self.mfcc_dir100       = \"{}/{}/mfcc\".format(root, \"train-clean-100\")\n",
        "          # TODO: Transcripts directory - use partition to acces train/dev directories from kaggle data using root\n",
        "        self.transcript_dir100 = \"{}/{}/transcript\".format(root, \"train-clean-100\")\n",
        "\n",
        "\n",
        "          # TODO: List files in sefl.mfcc_dir using os.listdir in sorted order\n",
        "        mfcc_names100          = sorted(os.listdir(self.mfcc_dir100))\n",
        "          # TODO: List files in self.transcript_dir using os.listdir in sorted order\n",
        "        transcript_names100    = sorted(os.listdir(self.transcript_dir100))\n",
        "\n",
        "        # TODO: List files in sefl.mfcc_dir using os.listdir in sorted order\n",
        "        mfcc_names          = sorted(os.listdir(self.mfcc_dir))\n",
        "        # TODO: List files in self.transcript_dir using os.listdir in sorted order\n",
        "        transcript_names    = sorted(os.listdir(self.transcript_dir))\n",
        "\n",
        "        # Making sure that we have the same no. of mfcc and transcripts\n",
        "        assert len(mfcc_names) == len(transcript_names)\n",
        "        assert len(mfcc_names100) == len(transcript_names100)\n",
        "        self.mfccs, self.transcripts = [], []\n",
        "\n",
        "        # TODO: Iterate through mfccs and transcripts\n",
        "        for i in range(len(mfcc_names)+len(mfcc_names100)-1):\n",
        "            #   Load a single mfcc\n",
        "            #   Load the corresponding transcript\n",
        "            if i < len(mfcc_names):\n",
        "              mfcc        = np.load(self.mfcc_dir+\"/\"+mfcc_names[i])\n",
        "\n",
        "              transcript  = np.load(self.transcript_dir+\"/\"+transcript_names[i])\n",
        "            else:\n",
        "              i = i-len(mfcc_names)\n",
        "              mfcc        = np.load(self.mfcc_dir100+\"/\"+mfcc_names100[i])\n",
        "              transcript  = np.load(self.transcript_dir100+\"/\"+transcript_names100[i])\n",
        "\n",
        "        #   Do Cepstral Normalization of mfcc (explained in writeup)\n",
        "            mfcc        = (mfcc - mfcc.mean(axis=0))/mfcc.std(axis=0)\n",
        "\n",
        "            # Remove [SOS] and [EOS] from the transcript\n",
        "            # (Is there an efficient way to do this without traversing through the transcript?)\n",
        "            # Note that SOS will always be in the starting and EOS at end, as the name suggests.\n",
        "            start_idx = np.where(transcript=='[SOS]')[0][-1]\n",
        "            end_idx = np.where(transcript=='[EOS]')[0][0]\n",
        "            transcript = transcript[start_idx+1:end_idx]\n",
        "        #   Append each mfcc to self.mfcc, transcript to self.transcript\n",
        "            self.mfccs.append(mfcc)\n",
        "            self.transcripts.append(transcript)\n",
        "\n",
        "        # TODO: Concatenate all mfccs in self.mfccs such that\n",
        "        # the final shape is T x 27 (Where T = T1 + T2 + ...)\n",
        "        self.mfccs          = np.concatenate(self.mfccs)\n",
        "        self.mfccs.astype(np.float32)\n",
        "\n",
        "        # TODO: Concatenate all transcripts in self.transcripts such that\n",
        "        # the final shape is (T,) meaning, each time step has one phoneme output\n",
        "        self.transcripts    = np.concatenate(self.transcripts)\n",
        "        #self.transcripts.astype(np.uint8)\n",
        "        # Hint: Use numpy to concatenate\n",
        "\n",
        "        # Length of the dataset is now the length of concatenated mfccs/transcripts\n",
        "        self.length = len(self.mfccs)\n",
        "\n",
        "        # Take some time to think about what we have done.\n",
        "        # self.mfcc is an array of the format (Frames x Features).\n",
        "        # Our goal is to recognize phonemes of each frame\n",
        "        # From hw0, you will be knowing what context is.\n",
        "        # We can introduce context by padding zeros on top and bottom of self.mfcc\n",
        "        self.mfccs = np.pad(self.mfccs, ((self.context, self.context), (0, 0)), 'constant', constant_values=(0, 0))\n",
        "\n",
        "        # The available phonemes in the transcript are of string data type\n",
        "        # But the neural network cannot predict strings as such.\n",
        "        # Hence, we map these phonemes to integers\n",
        "\n",
        "        # TODO: Map the phonemes to their corresponding list indexes in self.phonemes\n",
        "        #mapping_dic = {p:phonemes.index(p) for p in phonemes}\n",
        "        #unique_ph, seq = np.unique(self.transcripts, return_inverse = True)\n",
        "        self.transcripts = np.array([phonemes.index(p) for p in self.transcripts]).reshape(self.transcripts.shape)\n",
        "        self.transcripts.astype(np.uint8)\n",
        "        print(self.mfccs.shape, self.transcripts.shape)\n",
        "        # Now, if an element in self.transcript is 0, it means that it is 'SIL' (as per the above example)\n",
        "\n",
        "    def __len__(self):\n",
        "        return self.length\n",
        "\n",
        "    def __getitem__(self, ind):\n",
        "\n",
        "        # TODO: Based on context and offset, return a frame at given index with context frames to the left, and right.\n",
        "        frames = self.mfccs[ind:(ind+2*self.context+1)]\n",
        "        # After slicing, you get an array of shape 2*context+1 x 27. But our MLP needs 1d data and not 2d.\n",
        "        frames = frames.flatten() # TODO: Flatten to get 1d data\n",
        "\n",
        "        frames      = torch.FloatTensor(frames) # Convert to tensors\n",
        "        phonemes    = torch.tensor(self.transcripts[ind])\n",
        "\n",
        "        return frames, phonemes"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qNacQ8bpt9nw"
      },
      "source": [
        "# Parameters Configuration"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WE7tsinAuLNy"
      },
      "source": [
        "Storing your parameters and hyperparameters in a single configuration dictionary makes it easier to keep track of them during each experiment. It can also be used with weights and biases to log your parameters for each experiment and keep track of them across multiple experiments."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "PmKwlFqgt_Zq"
      },
      "outputs": [],
      "source": [
        "config_baseline = {\n",
        "    'root'          :\"/content/data/11-785-s23-hw1p2\",\n",
        "    'epochs'        : 5,\n",
        "    'batch_size'    : 1024,\n",
        "    'context'       : 20,\n",
        "    'init_lr'       : 1e-3,\n",
        "    'architecture'  : 'very-low-cutoff'\n",
        "    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n",
        "}\n",
        "\n",
        "\n",
        "config_queen = {\n",
        "    'root'          :\"/content/data/11-785-s23-hw1p2\",\n",
        "    'epochs'        : 30,\n",
        "    'batch_size'    : 8192,\n",
        "    'context'       : 25,\n",
        "    'init_lr'       : 1e-3,\n",
        "    'architecture'  : '8_layer_pyramid',\n",
        "    'dropout'       : 0.35,\n",
        "    'wdecay'        : 0.005,\n",
        "    'hidden_size'   : [2048, 2048, 1024, 1024, 1024, 1024, 1024, 1024],\n",
        "    'num_col'       : 27\n",
        "    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n",
        "}\n",
        "\n",
        "config_king = {\n",
        "    'root'          :\"/content/data/11-785-s23-hw1p2\",\n",
        "    'epochs'        : 30,\n",
        "    'batch_size'    : 16384, #  16384\n",
        "    'context'       : 30,\n",
        "    'init_lr'       : 1e-3,\n",
        "    'architecture'  : '6_layer_pyramid',\n",
        "    'dropout'       : 0.35,\n",
        "    'wdecay'        : 0.005,\n",
        "    'hidden_size'   : [2048, 2048, 2048, 2048, 1280, 1024],\n",
        "    'num_col'       : 27,\n",
        "    'scheduler_factor': 0.5,\n",
        "    'scheduler_patience': 3,\n",
        "    'scheduler_threshold': 0.0025,\n",
        "    # Add more as you need them - e.g dropout values, weight decay, scheduler parameters\n",
        "}\n",
        "\n",
        "config = config_king"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2mlwaKlDt_2c"
      },
      "source": [
        "# Create Datasets"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "7xi7V8x8W9z4",
        "outputId": "f20f8f94-eed6-485c-843a-3b6c236c5647"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "loading train 100\n"
          ]
        }
      ],
      "source": [
        "#TODO: Create a dataset object using the AudioDataset class for the training data\n",
        "if ONLY360:\n",
        "  train_data = AudioDataset(config['root'], phonemes = PHONEMES, context=config['context'], partition= \"train-clean-360\")\n",
        "  print(\"loading train 360\")\n",
        "elif ONLY100:\n",
        "  train_data = AudioDataset(config['root'], phonemes = PHONEMES, context=config['context'], partition= \"train-clean-100\")\n",
        "  print(\"loading train 100\")\n",
        "else:\n",
        "  train_data = AudioDatasetBoth(config['root'], phonemes = PHONEMES, context=config['context'])\n",
        "  print(\"loading train 360+100\")\n",
        "# TODO: Create a dataset object using the AudioDataset class for the validation data\n",
        "val_data = AudioDataset(config['root'], phonemes = PHONEMES, context=config['context'], partition= \"dev-clean\")\n",
        "\n",
        "# TODO: Create a dataset object using the AudioTestDataset class for the test data\n",
        "test_data = AudioTestDataset(config['root'], phonemes = PHONEMES, context=config['context'], partition= \"test-clean\")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "4mzoYfTKu14s",
        "outputId": "e1e15ecf-dfe1-4da5-b660-2ca9dd14b565"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Batch size     :  16384\n",
            "Context        :  30\n",
            "Input size     :  1647\n",
            "Output symbols :  40\n",
            "Hidden Layer Sizes :  [2048, 2048, 2048, 2048, 1280, 1024]\n",
            "Train dataset samples = 36091157, batches = 2203\n",
            "Validation dataset samples = 1928204, batches = 118\n",
            "Test dataset samples = 1934138, batches = 119\n"
          ]
        }
      ],
      "source": [
        "# Define dataloaders for train, val and test datasets\n",
        "# Dataloaders will yield a batch of frames and phonemes of given batch_size at every iteration\n",
        "# We shuffle train dataloader but not val & test dataloader. Why?\n",
        "\n",
        "train_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = train_data,\n",
        "    num_workers = 4,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = True\n",
        ")\n",
        "\n",
        "val_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = val_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "test_loader = torch.utils.data.DataLoader(\n",
        "    dataset     = test_data,\n",
        "    num_workers = 2,\n",
        "    batch_size  = config['batch_size'],\n",
        "    pin_memory  = True,\n",
        "    shuffle     = False\n",
        ")\n",
        "\n",
        "\n",
        "print(\"Batch size     : \", config['batch_size'])\n",
        "print(\"Context        : \", config['context'])\n",
        "print(\"Input size     : \", (2*config['context']+1)*27)\n",
        "print(\"Output symbols : \", len(PHONEMES)-2)\n",
        "if 'hidden_size' in config.keys():\n",
        "  print(\"Hidden Layer Sizes : \", config['hidden_size'])\n",
        "\n",
        "print(\"Train dataset samples = {}, batches = {}\".format(train_data.__len__(), len(train_loader)))\n",
        "print(\"Validation dataset samples = {}, batches = {}\".format(val_data.__len__(), len(val_loader)))\n",
        "print(\"Test dataset samples = {}, batches = {}\".format(test_data.__len__(), len(test_loader)))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "n-GV3UvgLSoF",
        "outputId": "1a1a1d6d-8440-4fb7-a8f0-2e21b324de98"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "torch.Size([16384, 1647]) torch.Size([16384])\n"
          ]
        }
      ],
      "source": [
        "# Testing code to check if your data loaders are working\n",
        "for i, data in enumerate(train_loader):\n",
        "    frames, phoneme = data\n",
        "    print(frames.shape, phoneme.shape)\n",
        "    break"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Nxjwve20JRJ2"
      },
      "source": [
        "# Network Architecture\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3NJzT-mRw6iy"
      },
      "source": [
        "This section defines your network architecture for the homework. We have given you a sample architecture that can easily clear the very low cutoff for the early submission deadline."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "OvcpontXQq9j"
      },
      "outputs": [],
      "source": [
        "# This architecture will make you cross the very low cutoff\n",
        "# However, you need to run a lot of experiments to cross the medium or high cutoff\n",
        "class NetworkBaseline(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, output_size):\n",
        "\n",
        "        super(NetworkBaseline, self).__init__()\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            torch.nn.Linear(input_size, 512),\n",
        "            torch.nn.ReLU(),\n",
        "            torch.nn.Linear(512, output_size)\n",
        "        )\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "4oCeyqKQhn2D"
      },
      "outputs": [],
      "source": [
        "# This architecture is used for the 8 layer 2048x2-1024x6 architecture, alternatively adding dropout layer\n",
        "\n",
        "class NetworkQueen(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, droprate):\n",
        "\n",
        "        super(NetworkQueen, self).__init__()\n",
        "        # create hidden layers, note the first size is input size, so layer number is one less\n",
        "        layer_num = len(hidden_sizes)\n",
        "        hidden_sizes = [input_size]+hidden_sizes # add input layer\n",
        "        hidden_layers = []\n",
        "        for l in range(layer_num):\n",
        "\n",
        "          hidden_layers.append(torch.nn.Linear(hidden_sizes[l], hidden_sizes[l+1]))\n",
        "          hidden_layers.append(torch.nn.BatchNorm1d(hidden_sizes[l+1]))\n",
        "          hidden_layers.append(torch.nn.SiLU())\n",
        "          if l % 2 == 0:\n",
        "            hidden_layers.append(torch.nn.Dropout(droprate))\n",
        "        # for the last layer to output, no activation need in this model\n",
        "        hidden_layers.append(torch.nn.Linear(hidden_sizes[-1], output_size))\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            *hidden_layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "yCX46pqVyiu6"
      },
      "outputs": [],
      "source": [
        "# This architecture is used for the 2048x4-1280-1024 architecture, adding dropout for every layer with 2048 neurons\n",
        "\n",
        "class NetworkKing(torch.nn.Module):\n",
        "\n",
        "    def __init__(self, input_size, hidden_sizes, output_size, droprate):\n",
        "\n",
        "        super(NetworkKing, self).__init__()\n",
        "        # create hidden layers, note the first size is input size, so layer number is one less\n",
        "        layer_num = len(hidden_sizes)\n",
        "        hidden_sizes = [input_size]+hidden_sizes # add input layer\n",
        "        hidden_layers = []\n",
        "        for l in range(layer_num):\n",
        "\n",
        "          hidden_layers.append(torch.nn.Linear(hidden_sizes[l], hidden_sizes[l+1]))\n",
        "          hidden_layers.append(torch.nn.BatchNorm1d(hidden_sizes[l+1]))\n",
        "          hidden_layers.append(torch.nn.SiLU())\n",
        "          if hidden_sizes[l+1] == 2048:\n",
        "            hidden_layers.append(torch.nn.Dropout(droprate))\n",
        "        # for the last layer to output, no activation need in this model\n",
        "        hidden_layers.append(torch.nn.Dropout(droprate-0.2))\n",
        "        hidden_layers.append(torch.nn.Linear(hidden_sizes[-1], output_size))\n",
        "\n",
        "        self.model = torch.nn.Sequential(\n",
        "            *hidden_layers)\n",
        "\n",
        "\n",
        "    def forward(self, x):\n",
        "        out = self.model(x)\n",
        "\n",
        "        return out"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "_4zHXTaSz8ez"
      },
      "outputs": [],
      "source": [
        "# weights initialization\n",
        "def initialize_weights(m):\n",
        "    if isinstance(m, torch.nn.Linear):\n",
        "        torch.nn.init.kaiming_normal_(m.weight.data)\n",
        "#       torch.nn.init.kaiming_uniform_(m.weight.data)\n",
        "    elif isinstance(m, torch.nn.BatchNorm1d):\n",
        "        torch.nn.init.constant_(m.weight.data, 1)\n",
        "        torch.nn.init.constant_(m.bias.data, 0)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "HejoSXe3vMVU"
      },
      "source": [
        "# Define Model, Loss Function and Optimizer"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "xAhGBH7-xxth"
      },
      "source": [
        "Here we define the model, loss function, optimizer and optionally a learning rate scheduler."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 1000
        },
        "id": "_qtrEM1ZvLje",
        "outputId": "44b0403b-3e6c-42af-d870-4c4052ea1d3d"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "==========================================================================\n",
            "                         Kernel Shape   Output Shape     Params  Mult-Adds\n",
            "Layer                                                                     \n",
            "0_model.Linear_0         [1647, 2048]  [16384, 2048]  3.375104M  3.373056M\n",
            "1_model.BatchNorm1d_1          [2048]  [16384, 2048]     4.096k     2.048k\n",
            "2_model.SiLU_2                      -  [16384, 2048]          -          -\n",
            "3_model.Dropout_3                   -  [16384, 2048]          -          -\n",
            "4_model.Linear_4         [2048, 2048]  [16384, 2048]  4.196352M  4.194304M\n",
            "5_model.BatchNorm1d_5          [2048]  [16384, 2048]     4.096k     2.048k\n",
            "6_model.SiLU_6                      -  [16384, 2048]          -          -\n",
            "7_model.Dropout_7                   -  [16384, 2048]          -          -\n",
            "8_model.Linear_8         [2048, 2048]  [16384, 2048]  4.196352M  4.194304M\n",
            "9_model.BatchNorm1d_9          [2048]  [16384, 2048]     4.096k     2.048k\n",
            "10_model.SiLU_10                    -  [16384, 2048]          -          -\n",
            "11_model.Dropout_11                 -  [16384, 2048]          -          -\n",
            "12_model.Linear_12       [2048, 2048]  [16384, 2048]  4.196352M  4.194304M\n",
            "13_model.BatchNorm1d_13        [2048]  [16384, 2048]     4.096k     2.048k\n",
            "14_model.SiLU_14                    -  [16384, 2048]          -          -\n",
            "15_model.Dropout_15                 -  [16384, 2048]          -          -\n",
            "16_model.Linear_16       [2048, 1280]  [16384, 1280]   2.62272M   2.62144M\n",
            "17_model.BatchNorm1d_17        [1280]  [16384, 1280]      2.56k      1.28k\n",
            "18_model.SiLU_18                    -  [16384, 1280]          -          -\n",
            "19_model.Linear_19       [1280, 1024]  [16384, 1024]  1.311744M   1.31072M\n",
            "20_model.BatchNorm1d_20        [1024]  [16384, 1024]     2.048k     1.024k\n",
            "21_model.SiLU_21                    -  [16384, 1024]          -          -\n",
            "22_model.Dropout_22                 -  [16384, 1024]          -          -\n",
            "23_model.Linear_23         [1024, 40]    [16384, 40]      41.0k     40.96k\n",
            "--------------------------------------------------------------------------\n",
            "                          Totals\n",
            "Total params          19.960616M\n",
            "Trainable params      19.960616M\n",
            "Non-trainable params         0.0\n",
            "Mult-Adds             19.939584M\n",
            "==========================================================================\n"
          ]
        },
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.8/dist-packages/torchsummaryX/torchsummaryX.py:101: FutureWarning: Dropping of nuisance columns in DataFrame reductions (with 'numeric_only=None') is deprecated; in a future version this will raise TypeError.  Select only valid columns before calling the reduction.\n",
            "  df_sum = df.sum()\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "\n",
              "  <div id=\"df-cae74634-2e3d-41c0-8f93-5bbaa9ede58d\">\n",
              "    <div class=\"colab-df-container\">\n",
              "      <div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>Kernel Shape</th>\n",
              "      <th>Output Shape</th>\n",
              "      <th>Params</th>\n",
              "      <th>Mult-Adds</th>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>Layer</th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "      <th></th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0_model.Linear_0</th>\n",
              "      <td>[1647, 2048]</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>3375104.0</td>\n",
              "      <td>3373056.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1_model.BatchNorm1d_1</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2_model.SiLU_2</th>\n",
              "      <td>-</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3_model.Dropout_3</th>\n",
              "      <td>-</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4_model.Linear_4</th>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>4196352.0</td>\n",
              "      <td>4194304.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5_model.BatchNorm1d_5</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6_model.SiLU_6</th>\n",
              "      <td>-</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7_model.Dropout_7</th>\n",
              "      <td>-</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8_model.Linear_8</th>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>4196352.0</td>\n",
              "      <td>4194304.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9_model.BatchNorm1d_9</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10_model.SiLU_10</th>\n",
              "      <td>-</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11_model.Dropout_11</th>\n",
              "      <td>-</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12_model.Linear_12</th>\n",
              "      <td>[2048, 2048]</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>4196352.0</td>\n",
              "      <td>4194304.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13_model.BatchNorm1d_13</th>\n",
              "      <td>[2048]</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>4096.0</td>\n",
              "      <td>2048.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14_model.SiLU_14</th>\n",
              "      <td>-</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15_model.Dropout_15</th>\n",
              "      <td>-</td>\n",
              "      <td>[16384, 2048]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16_model.Linear_16</th>\n",
              "      <td>[2048, 1280]</td>\n",
              "      <td>[16384, 1280]</td>\n",
              "      <td>2622720.0</td>\n",
              "      <td>2621440.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17_model.BatchNorm1d_17</th>\n",
              "      <td>[1280]</td>\n",
              "      <td>[16384, 1280]</td>\n",
              "      <td>2560.0</td>\n",
              "      <td>1280.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18_model.SiLU_18</th>\n",
              "      <td>-</td>\n",
              "      <td>[16384, 1280]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19_model.Linear_19</th>\n",
              "      <td>[1280, 1024]</td>\n",
              "      <td>[16384, 1024]</td>\n",
              "      <td>1311744.0</td>\n",
              "      <td>1310720.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20_model.BatchNorm1d_20</th>\n",
              "      <td>[1024]</td>\n",
              "      <td>[16384, 1024]</td>\n",
              "      <td>2048.0</td>\n",
              "      <td>1024.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21_model.SiLU_21</th>\n",
              "      <td>-</td>\n",
              "      <td>[16384, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22_model.Dropout_22</th>\n",
              "      <td>-</td>\n",
              "      <td>[16384, 1024]</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23_model.Linear_23</th>\n",
              "      <td>[1024, 40]</td>\n",
              "      <td>[16384, 40]</td>\n",
              "      <td>41000.0</td>\n",
              "      <td>40960.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>\n",
              "      <button class=\"colab-df-convert\" onclick=\"convertToInteractive('df-cae74634-2e3d-41c0-8f93-5bbaa9ede58d')\"\n",
              "              title=\"Convert this dataframe to an interactive table.\"\n",
              "              style=\"display:none;\">\n",
              "        \n",
              "  <svg xmlns=\"http://www.w3.org/2000/svg\" height=\"24px\"viewBox=\"0 0 24 24\"\n",
              "       width=\"24px\">\n",
              "    <path d=\"M0 0h24v24H0V0z\" fill=\"none\"/>\n",
              "    <path d=\"M18.56 5.44l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94zm-11 1L8.5 8.5l.94-2.06 2.06-.94-2.06-.94L8.5 2.5l-.94 2.06-2.06.94zm10 10l.94 2.06.94-2.06 2.06-.94-2.06-.94-.94-2.06-.94 2.06-2.06.94z\"/><path d=\"M17.41 7.96l-1.37-1.37c-.4-.4-.92-.59-1.43-.59-.52 0-1.04.2-1.43.59L10.3 9.45l-7.72 7.72c-.78.78-.78 2.05 0 2.83L4 21.41c.39.39.9.59 1.41.59.51 0 1.02-.2 1.41-.59l7.78-7.78 2.81-2.81c.8-.78.8-2.07 0-2.86zM5.41 20L4 18.59l7.72-7.72 1.47 1.35L5.41 20z\"/>\n",
              "  </svg>\n",
              "      </button>\n",
              "      \n",
              "  <style>\n",
              "    .colab-df-container {\n",
              "      display:flex;\n",
              "      flex-wrap:wrap;\n",
              "      gap: 12px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert {\n",
              "      background-color: #E8F0FE;\n",
              "      border: none;\n",
              "      border-radius: 50%;\n",
              "      cursor: pointer;\n",
              "      display: none;\n",
              "      fill: #1967D2;\n",
              "      height: 32px;\n",
              "      padding: 0 0 0 0;\n",
              "      width: 32px;\n",
              "    }\n",
              "\n",
              "    .colab-df-convert:hover {\n",
              "      background-color: #E2EBFA;\n",
              "      box-shadow: 0px 1px 2px rgba(60, 64, 67, 0.3), 0px 1px 3px 1px rgba(60, 64, 67, 0.15);\n",
              "      fill: #174EA6;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert {\n",
              "      background-color: #3B4455;\n",
              "      fill: #D2E3FC;\n",
              "    }\n",
              "\n",
              "    [theme=dark] .colab-df-convert:hover {\n",
              "      background-color: #434B5C;\n",
              "      box-shadow: 0px 1px 3px 1px rgba(0, 0, 0, 0.15);\n",
              "      filter: drop-shadow(0px 1px 2px rgba(0, 0, 0, 0.3));\n",
              "      fill: #FFFFFF;\n",
              "    }\n",
              "  </style>\n",
              "\n",
              "      <script>\n",
              "        const buttonEl =\n",
              "          document.querySelector('#df-cae74634-2e3d-41c0-8f93-5bbaa9ede58d button.colab-df-convert');\n",
              "        buttonEl.style.display =\n",
              "          google.colab.kernel.accessAllowed ? 'block' : 'none';\n",
              "\n",
              "        async function convertToInteractive(key) {\n",
              "          const element = document.querySelector('#df-cae74634-2e3d-41c0-8f93-5bbaa9ede58d');\n",
              "          const dataTable =\n",
              "            await google.colab.kernel.invokeFunction('convertToInteractive',\n",
              "                                                     [key], {});\n",
              "          if (!dataTable) return;\n",
              "\n",
              "          const docLinkHtml = 'Like what you see? Visit the ' +\n",
              "            '<a target=\"_blank\" href=https://colab.research.google.com/notebooks/data_table.ipynb>data table notebook</a>'\n",
              "            + ' to learn more about interactive tables.';\n",
              "          element.innerHTML = '';\n",
              "          dataTable['output_type'] = 'display_data';\n",
              "          await google.colab.output.renderOutput(dataTable, element);\n",
              "          const docLink = document.createElement('div');\n",
              "          docLink.innerHTML = docLinkHtml;\n",
              "          element.appendChild(docLink);\n",
              "        }\n",
              "      </script>\n",
              "    </div>\n",
              "  </div>\n",
              "  "
            ],
            "text/plain": [
              "                         Kernel Shape   Output Shape     Params  Mult-Adds\n",
              "Layer                                                                     \n",
              "0_model.Linear_0         [1647, 2048]  [16384, 2048]  3375104.0  3373056.0\n",
              "1_model.BatchNorm1d_1          [2048]  [16384, 2048]     4096.0     2048.0\n",
              "2_model.SiLU_2                      -  [16384, 2048]        NaN        NaN\n",
              "3_model.Dropout_3                   -  [16384, 2048]        NaN        NaN\n",
              "4_model.Linear_4         [2048, 2048]  [16384, 2048]  4196352.0  4194304.0\n",
              "5_model.BatchNorm1d_5          [2048]  [16384, 2048]     4096.0     2048.0\n",
              "6_model.SiLU_6                      -  [16384, 2048]        NaN        NaN\n",
              "7_model.Dropout_7                   -  [16384, 2048]        NaN        NaN\n",
              "8_model.Linear_8         [2048, 2048]  [16384, 2048]  4196352.0  4194304.0\n",
              "9_model.BatchNorm1d_9          [2048]  [16384, 2048]     4096.0     2048.0\n",
              "10_model.SiLU_10                    -  [16384, 2048]        NaN        NaN\n",
              "11_model.Dropout_11                 -  [16384, 2048]        NaN        NaN\n",
              "12_model.Linear_12       [2048, 2048]  [16384, 2048]  4196352.0  4194304.0\n",
              "13_model.BatchNorm1d_13        [2048]  [16384, 2048]     4096.0     2048.0\n",
              "14_model.SiLU_14                    -  [16384, 2048]        NaN        NaN\n",
              "15_model.Dropout_15                 -  [16384, 2048]        NaN        NaN\n",
              "16_model.Linear_16       [2048, 1280]  [16384, 1280]  2622720.0  2621440.0\n",
              "17_model.BatchNorm1d_17        [1280]  [16384, 1280]     2560.0     1280.0\n",
              "18_model.SiLU_18                    -  [16384, 1280]        NaN        NaN\n",
              "19_model.Linear_19       [1280, 1024]  [16384, 1024]  1311744.0  1310720.0\n",
              "20_model.BatchNorm1d_20        [1024]  [16384, 1024]     2048.0     1024.0\n",
              "21_model.SiLU_21                    -  [16384, 1024]        NaN        NaN\n",
              "22_model.Dropout_22                 -  [16384, 1024]        NaN        NaN\n",
              "23_model.Linear_23         [1024, 40]    [16384, 40]    41000.0    40960.0"
            ]
          },
          "execution_count": 21,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "# INPUT_SIZE  = (2*config['context'] + 1) * 27 # Why is this the case?\n",
        "# frame is flatter, so total 27 cols per row, with 2*context+1 rows\n",
        "# model_baseline       = NetworkBaseline(INPUT_SIZE, len(train_data.phonemes)).to(device)\n",
        "model_queen = NetworkKing(\n",
        "    (2*config['context']+1)*config['num_col'],\n",
        "    config['hidden_size'],\n",
        "    len(train_data.phonemes)-2,\n",
        "    config['dropout']).to(device)\n",
        "model_queen.apply(initialize_weights)\n",
        "summary(model_queen, frames.to(device))\n",
        "# Check number of parameters of your network\n",
        "# Remember, you are limited to 20 million parameters for HW1 (including ensembles)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "UROGEVJevKD-"
      },
      "outputs": [],
      "source": [
        "criterion = torch.nn.CrossEntropyLoss() # Defining Loss function.\n",
        "# We use CE because the task is multi-class classification\n",
        "\n",
        "#optimizer = torch.optim.Adam(model09.parameters(), lr= config['init_lr']) #Defining Optimizer\n",
        "# Recommended : Define Scheduler for Learning Rate,\n",
        "# including but not limited to StepLR, MultiStepLR, CosineAnnealingLR, ReduceLROnPlateau, etc.\n",
        "# You can refer to Pytorch documentation for more information on how to use them.\n",
        "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2,4], gamma=0.1)\n",
        "# scheduler2 = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='min', factor=0.1, patience=2)\n",
        "\n",
        "# Is your training time very high?\n",
        "# Look into mixed precision training if your GPU (Tesla T4, V100, etc) can make use of it\n",
        "# Refer - https://pytorch.org/docs/stable/notes/amp_examples.html\n",
        "\n",
        "optimizer = torch.optim.AdamW(model_queen.parameters(), lr= config['init_lr'], weight_decay=config['wdecay']) # AdamW - w decay\n",
        "\n",
        "# Recommended : Define Scheduler for Learning Rate,\n",
        "# including but not limited to StepLR, MultiStepLR, CosineAnnealingLR, ReduceLROnPlateau, etc.\n",
        "# You can refer to Pytorch documentation for more information on how to use them.\n",
        "#scheduler = torch.optim.lr_scheduler.MultiStepLR(optimizer, milestones=[2, 4, 6, 8, 10, 12, 14, 16, 18], gamma=0.75)\n",
        "scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(\n",
        "    optimizer, mode='max',\n",
        "    factor=config['scheduler_factor'],\n",
        "    patience=config['scheduler_patience'],\n",
        "    threshold=config['scheduler_threshold'],\n",
        "    verbose=True)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "IBwunYpyugFg"
      },
      "source": [
        "# Training and Validation Functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "1JgeNhx4x2-P"
      },
      "source": [
        "This section covers the training, and validation functions for each epoch of running your experiment with a given model architecture. The code has been provided to you, but we recommend going through the comments to understand the workflow to enable you to write these loops for future HWs."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "XblOHEVtKab2",
        "outputId": "fa1fc3ea-219f-49e9-9fb9-b2a7d82a3ac0"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "273"
            ]
          },
          "execution_count": 23,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "torch.cuda.empty_cache()\n",
        "gc.collect()"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "8wjPz7DHqKcL"
      },
      "outputs": [],
      "source": [
        "def train(model, dataloader, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "\n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "\n",
        "        ### Initialize Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        ### Move Data to Device (Ideally GPU)\n",
        "        frames      = frames.to(device)\n",
        "        phonemes    = phonemes.to(device)\n",
        "\n",
        "        ### Forward Propagation\n",
        "        logits  = model(frames)\n",
        "\n",
        "        ### Loss Calculation\n",
        "        loss    = criterion(logits, phonemes)\n",
        "\n",
        "        ### Backward Propagation\n",
        "        loss.backward()\n",
        "\n",
        "        ### Gradient Descent\n",
        "        optimizer.step()\n",
        "\n",
        "        tloss   += loss.item()\n",
        "        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    tloss   /= len(train_loader)\n",
        "    tacc    /= len(train_loader)\n",
        "\n",
        "    return tloss, tacc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "N8GnDX09Qlb3"
      },
      "outputs": [],
      "source": [
        "# Using mix precision\n",
        "def train_mp(model, dataloader, optimizer, criterion):\n",
        "\n",
        "    model.train()\n",
        "    tloss, tacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(train_loader), dynamic_ncols=True, leave=False, position=0, desc='Train')\n",
        "    scaler = torch.cuda.amp.GradScaler()\n",
        "\n",
        "    # for i, (frames, phonemes) in enumerate(dataloader):\n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "        # print(frames.shape)\n",
        "        # print(phonemes.shape)\n",
        "        ### Initialize Gradients\n",
        "        optimizer.zero_grad()\n",
        "\n",
        "        ### Move Data to Device (Ideally GPU)\n",
        "        frames      = frames.to(device)\n",
        "        phonemes    = phonemes.to(device)\n",
        "\n",
        "        with torch.autocast(device_type='cuda', dtype=torch.float16):\n",
        "            logits = model(frames)\n",
        "            loss = criterion(logits, phonemes)\n",
        "\n",
        "        scaler.scale(loss).backward()\n",
        "\n",
        "        scaler.step(optimizer)\n",
        "\n",
        "        scaler.update()\n",
        "\n",
        "        tloss   += loss.item()\n",
        "        tacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(tloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(tacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    tloss   /= len(train_loader)\n",
        "    tacc    /= len(train_loader)\n",
        "    #wandb.log({'train_acc': tacc*100, 'train_loss': tloss})\n",
        "    return tloss, tacc"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Q5npQNFH315V"
      },
      "outputs": [],
      "source": [
        "def eval(model, dataloader):\n",
        "\n",
        "    model.eval() # set model in evaluation mode\n",
        "    vloss, vacc = 0, 0 # Monitoring loss and accuracy\n",
        "    batch_bar   = tqdm(total=len(val_loader), dynamic_ncols=True, position=0, leave=False, desc='Val')\n",
        "\n",
        "    for i, (frames, phonemes) in enumerate(dataloader):\n",
        "\n",
        "        ### Move data to device (ideally GPU)\n",
        "        frames      = frames.to(device)\n",
        "        phonemes    = phonemes.to(device)\n",
        "\n",
        "        # makes sure that there are no gradients computed as we are not training the model now\n",
        "        with torch.inference_mode():\n",
        "            ### Forward Propagation\n",
        "            logits  = model(frames)\n",
        "            ### Loss Calculation\n",
        "            loss    = criterion(logits, phonemes)\n",
        "\n",
        "        vloss   += loss.item()\n",
        "        vacc    += torch.sum(torch.argmax(logits, dim= 1) == phonemes).item()/logits.shape[0]\n",
        "\n",
        "        # Do you think we need loss.backward() and optimizer.step() here?\n",
        "\n",
        "        batch_bar.set_postfix(loss=\"{:.04f}\".format(float(vloss / (i + 1))),\n",
        "                              acc=\"{:.04f}%\".format(float(vacc*100 / (i + 1))))\n",
        "        batch_bar.update()\n",
        "\n",
        "        ### Release memory\n",
        "        del frames, phonemes, logits\n",
        "        torch.cuda.empty_cache()\n",
        "\n",
        "    batch_bar.close()\n",
        "    vloss   /= len(val_loader)\n",
        "    vacc    /= len(val_loader)\n",
        "\n",
        "    return vloss, vacc"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yMd_XxPku5qp"
      },
      "source": [
        "# Weights and Biases Setup"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "tjIbhR1wwbgI"
      },
      "source": [
        "This section is to enable logging metrics and files with Weights and Biases. Please refer to wandb documentationa and recitation 0 that covers the use of weights and biases for logging, hyperparameter tuning and monitoring your runs for your homeworks. Using this tool makes it very easy to show results when submitting your code and models for homeworks, and also extremely useful for study groups to organize and run ablations under a single team in wandb.\n",
        "\n",
        "We have written code for you to make use of it out of the box, so that you start using wandb for all your HWs from the beginning."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCDYx5VEu6qI",
        "outputId": "d3fb9377-733b-4899-8d5c-2f2632775f70"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: W&B API key is configured. Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m If you're specifying your api key in code, ensure this code is not shared publicly.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: \u001b[33mWARNING\u001b[0m Consider setting the WANDB_API_KEY environment variable, or running `wandb login` from the command line.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Appending key for api.wandb.ai to your netrc file: /root/.netrc\n"
          ]
        },
        {
          "data": {
            "text/plain": [
              "True"
            ]
          },
          "execution_count": 27,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "wandb.login(key=\"27ad915a9386068b1fc160cd97b84be7ba1fe659\") #API Key is in your wandb account, under settings (wandb.ai/settings)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 142
        },
        "id": "xvUnYd3Bw2up",
        "outputId": "6e79ad98-aeeb-45e2-ac71-16dbf1d4c7ac"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwenxinz3\u001b[0m (\u001b[33msharonxin1207\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230224_150919-fo4quypo</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sharonxin1207/hw1p2/runs/fo4quypo' target=\"_blank\">test-run</a></strong> to <a href='https://wandb.ai/sharonxin1207/hw1p2' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/sharonxin1207/hw1p2' target=\"_blank\">https://wandb.ai/sharonxin1207/hw1p2</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/sharonxin1207/hw1p2/runs/fo4quypo' target=\"_blank\">https://wandb.ai/sharonxin1207/hw1p2/runs/fo4quypo</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "# Create your wandb run\n",
        "run = wandb.init(\n",
        "    name    = \"official-run\", ### Wandb creates random run names if you skip this field, we recommend you give useful names\n",
        "    reinit  = True, ### Allows reinitalizing runs when you re-run this cell\n",
        "    #id     = \"y28t31uz\", ### Insert specific run id here if you want to resume a previous run\n",
        "    #resume = \"must\", ### You need this to resume previous runs, but comment out reinit = True when using this\n",
        "    project = \"hw1p2\", ### Project should be created in your wandb account\n",
        "    config  = config ### Wandb Config for your run\n",
        ")"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wft15E_IxYFi",
        "outputId": "756e3cad-b35d-4e1d-8e7c-6bb78be8cffc"
      },
      "outputs": [
        {
          "data": {
            "text/plain": [
              "['/content/wandb/run-20230224_150919-fo4quypo/files/model_arch_official.txt']"
            ]
          },
          "execution_count": 29,
          "metadata": {},
          "output_type": "execute_result"
        }
      ],
      "source": [
        "### Save your model architecture as a string with str(model)\n",
        "model_arch  = str(model_queen)\n",
        "\n",
        "### Save it in a txt file\n",
        "arch_file   = open(\"model_arch_official.txt\", \"w\")\n",
        "file_write  = arch_file.write(model_arch)\n",
        "arch_file.close()\n",
        "\n",
        "### log it in your wandb run with wandb.save()\n",
        "wandb.save('model_arch_official.txt')"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Qu2lIxPnNU6-"
      },
      "source": [
        "## If reloading previous model"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "h2NK8CHKbtaa"
      },
      "outputs": [],
      "source": [
        "RELOAD_PATH = None # or model file to reload, such as 'Attempt460_finetune.pt'"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "lVJ01QdkRPW7"
      },
      "outputs": [],
      "source": [
        "def reload_prev_model(model_name):\n",
        "    model_queen = NetworkKing(\n",
        "        (2*config['context']+1)*config['num_col'],\n",
        "        config['hidden_size'],\n",
        "        40,\n",
        "        config['dropout']).to(device)\n",
        "    model_queen.apply(initialize_weights)\n",
        "    criterion = torch.nn.CrossEntropyLoss()\n",
        "    optimizer = torch.optim.AdamW(model_queen.parameters(), lr= config['init_lr']*0.05, weight_decay=config['wdecay']) # AdamW - w decay\n",
        "\n",
        "    scheduler = torch.optim.lr_scheduler.ReduceLROnPlateau(optimizer, mode='max', factor=0.5, patience=3, threshold=0.0025, verbose=True)\n",
        "    model_save_name = model_name #'Attempt460_finetune.pt'\n",
        "    path = F\"/content/{model_save_name}\"\n",
        "\n",
        "    checkpoint = torch.load(path)\n",
        "    model_queen.load_state_dict(checkpoint['model_state_dict'])\n",
        "    optimizer.load_state_dict(checkpoint['optimizer_state_dict'])\n",
        "\n",
        "    scheduler.load_state_dict(checkpoint['scheduler'])\n",
        "\n",
        "    return model_queen, optimizer, scheduler\n",
        "\n",
        "\n",
        "def save_model_state(model_path, model, optimizer, scheduler, train_loss, val_loss, val_acc):\n",
        "    model_save_name = model_path # 'Attempt460.pt'\n",
        "    path = F\"/content/drive/MyDrive/{model_save_name}\"\n",
        "    gcp_path = F\"/content/{model_save_name}\"\n",
        "    torch.save({\n",
        "                'model_state_dict': model.state_dict(),\n",
        "                'optimizer_state_dict': optimizer.state_dict(),\n",
        "                'scheduler': scheduler.state_dict(),\n",
        "                'train_loss': train_loss,\n",
        "                'val_loss':val_loss,\n",
        "                'val_acc': val_acc\n",
        "                }, gcp_path)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "Lmpbzljab19S"
      },
      "outputs": [],
      "source": [
        "if RELOAD_PATH is not None:\n",
        "  model_queen, optimizer, scheduler = reload_prev_model(RELOAD_PATH)"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nclx_04fu7Dd"
      },
      "source": [
        "# Experiment"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MdLMWfEpyGOB"
      },
      "source": [
        "Now, it is time to finally run your ablations! Have fun! --- not fun :("
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "MG4F77Nm0Am9"
      },
      "outputs": [],
      "source": [
        "# Iterate over number of epochs to train and evaluate your model\n",
        "torch.cuda.empty_cache()\n",
        "gc.collect()\n",
        "wandb.watch(model_queen, log=\"all\")\n",
        "\n",
        "for epoch in range(config['epochs']):\n",
        "\n",
        "    print(\"\\nEpoch {}/{}\".format(epoch+1, config['epochs']))\n",
        "\n",
        "    curr_lr                 = float(optimizer.param_groups[0]['lr'])\n",
        "    train_loss, train_acc   = train_mp(model_queen, train_loader, optimizer, criterion)\n",
        "    val_loss, val_acc       = eval(model_queen, val_loader)\n",
        "    scheduler.step(val_acc)\n",
        "    # scheduler.step()\n",
        "\n",
        "    print(\"\\tTrain Acc {:.04f}%\\tTrain Loss {:.04f}\\t Learning Rate {:.07f}\".format(train_acc*100, train_loss, curr_lr))\n",
        "    print(\"\\tVal Acc {:.04f}%\\tVal Loss {:.04f}\".format(val_acc*100, val_loss))\n",
        "    if (epoch+1) % 5 == 0:\n",
        "      model_save_name = \"Tuning\"+MODEL_SAVED_NAME\n",
        "      save_model_state(model_save_name, model_queen, optimizer, scheduler, train_loss, val_loss, val_acc)\n",
        "\n",
        "    ### Log metrics at each epoch in your run\n",
        "    # Optionally, you can log at each batch inside train/eval functions\n",
        "    # (explore wandb documentation/wandb recitation)\n",
        "    wandb.log({'train_acc': train_acc*100, 'train_loss': train_loss,\n",
        "               'val_acc': val_acc*100, 'valid_loss': val_loss, 'lr': curr_lr})\n",
        "\n",
        "    ### Highly Recommended: Save checkpoint in drive and/or wandb if accuracy is better than your current best\n"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 355,
          "referenced_widgets": [
            "894909de0f5d44b5b302001ae774be8a",
            "bb2bf458357e471ab97c07af5ebe0544",
            "3d491d5f793749fa81df3c61281a7c19",
            "5e79a744343740ad90a7590985f3f78d",
            "4335326e40164eacad76938839f5b1d3",
            "365817d30c914698a6e175273aaf8523",
            "729642819a1446f8bfd4127c9f27baa5",
            "7a0d1d45a0304b77a54c118dcd17c8d2"
          ]
        },
        "id": "k_UPZD8wQzrB",
        "outputId": "63f453d0-be5f-462c-e8e6-f8c42f2ffb38"
      },
      "outputs": [
        {
          "data": {
            "text/html": [
              "Waiting for W&B process to finish... <strong style=\"color:green\">(success).</strong>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "894909de0f5d44b5b302001ae774be8a",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "VBox(children=(Label(value='0.002 MB of 0.002 MB uploaded (0.000 MB deduped)\\r'), FloatProgress(value=1.0, max…"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "<style>\n",
              "    table.wandb td:nth-child(1) { padding: 0 10px; text-align: left ; width: auto;} td:nth-child(2) {text-align: left ; width: 100%}\n",
              "    .wandb-row { display: flex; flex-direction: row; flex-wrap: wrap; justify-content: flex-start; width: 100% }\n",
              "    .wandb-col { display: flex; flex-direction: column; flex-basis: 100%; flex: 1; padding: 10px; }\n",
              "    </style>\n",
              "<div class=\"wandb-row\"><div class=\"wandb-col\"><h3>Run history:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>█████▃▃▃▃▁</td></tr><tr><td>train_acc</td><td>▁▅▆▇▇▇████</td></tr><tr><td>train_loss</td><td>█▄▃▂▂▂▁▁▁▁</td></tr><tr><td>val_acc</td><td>▁▄▅▆▆▇▇███</td></tr><tr><td>valid_loss</td><td>█▅▄▃▃▂▂▁▁▁</td></tr></table><br/></div><div class=\"wandb-col\"><h3>Run summary:</h3><br/><table class=\"wandb\"><tr><td>lr</td><td>0.00025</td></tr><tr><td>train_acc</td><td>84.85594</td></tr><tr><td>train_loss</td><td>0.44112</td></tr><tr><td>val_acc</td><td>87.23075</td></tr><tr><td>valid_loss</td><td>0.36695</td></tr></table><br/></div></div>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run <strong style=\"color:#cdcd00\">10rd-run</strong> at: <a href='https://wandb.ai/sharonxin1207/hw1p2/runs/lsdeu9pt' target=\"_blank\">https://wandb.ai/sharonxin1207/hw1p2/runs/lsdeu9pt</a><br/>Synced 5 W&B file(s), 0 media file(s), 0 artifact file(s) and 1 other file(s)"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Find logs at: <code>./wandb/run-20230213_030459-lsdeu9pt/logs</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "### Finish your wandb run\n",
        "run.finish()\n",
        "model_save_name = MODEL_SAVED_NAME\n",
        "save_model_state(model_save_name, model_queen, optimizer, scheduler, train_loss, val_loss, val_acc)\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "_kXwf5YUo_4A"
      },
      "source": [
        "# Testing and submission to Kaggle"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WI1hSFYLpJvH"
      },
      "source": [
        "Before we get to the following code, make sure to see the format of submission given in *sample_submission.csv*. Once you have done so, it is time to fill the following function to complete your inference on test data. Refer the eval function from previous cells to get an idea of how to go about completing this function."
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "R-SU9fZ3xHtk"
      },
      "outputs": [],
      "source": [
        "def test(model, test_loader):\n",
        "    ### What you call for model to perform inference?\n",
        "    model.eval() # TODO >> eval\n",
        "\n",
        "    ### List to store predicted phonemes of test data\n",
        "    test_predictions = []\n",
        "\n",
        "    ### Which mode do you need to avoid gradients?\n",
        "    with torch.inference_mode(): # TODO\n",
        "\n",
        "        for i, mfccs in enumerate(tqdm(test_loader)):\n",
        "\n",
        "            mfccs   = mfccs.to(device)\n",
        "\n",
        "            logits  = model(mfccs)\n",
        "\n",
        "            ### Get most likely predicted phoneme with argmax\n",
        "            predicted_phonemes = torch.argmax(logits, dim = 1)\n",
        "            predicted_phonemes = [PHONEMES[p] for p in predicted_phonemes]\n",
        "\n",
        "            ### How do you store predicted_phonemes with test_predictions? Hint, look at eval\n",
        "            test_predictions.extend(predicted_phonemes)\n",
        "    # Map the numeric representation of phonemes back to string\n",
        "    # test_predictions = [PHONEMES[p] for p in test_predictions]\n",
        "\n",
        "    return test_predictions"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 191,
          "referenced_widgets": [
            "01f1202f35d44b67996d2e392c91541f",
            "523085f8915d46bc8f17cfd55a0a3403",
            "8c33fa3a84534075ab96a9b316cf40b0",
            "c6faf2a48ccf487aa6341fa92b8c932f",
            "6837c6f58eeb437ca2679aa071c464ee",
            "a8b56afc621247db8d59731b02a740e3",
            "03befaa3aa2247e08724aeab064069a9",
            "4d9ef737bdf94013a8df17caa89618e9",
            "ac5965e729624d238d053222a629c58f",
            "01a19f3cae8140f08100fd06c9bd5588",
            "30703581e2a44704b7706fcae6acf1a6"
          ]
        },
        "id": "wG9v6Xmxu7wp",
        "outputId": "82b3dd64-6156-4eba-c707-38cf42b1a941"
      },
      "outputs": [
        {
          "name": "stderr",
          "output_type": "stream",
          "text": [
            "ERROR:wandb.jupyter:Failed to detect the name of this notebook, you can set it manually with the WANDB_NOTEBOOK_NAME environment variable to enable code saving.\n",
            "\u001b[34m\u001b[1mwandb\u001b[0m: Currently logged in as: \u001b[33mwenxinz3\u001b[0m (\u001b[33msharonxin1207\u001b[0m). Use \u001b[1m`wandb login --relogin`\u001b[0m to force relogin\n"
          ]
        },
        {
          "data": {
            "text/html": [
              "Tracking run with wandb version 0.13.10"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Run data is saved locally in <code>/content/wandb/run-20230218_045941-c3sx98g1</code>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              "Syncing run <strong><a href='https://wandb.ai/sharonxin1207/uncategorized/runs/c3sx98g1' target=\"_blank\">bumbling-microwave-18</a></strong> to <a href='https://wandb.ai/sharonxin1207/uncategorized' target=\"_blank\">Weights & Biases</a> (<a href='https://wandb.me/run' target=\"_blank\">docs</a>)<br/>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View project at <a href='https://wandb.ai/sharonxin1207/uncategorized' target=\"_blank\">https://wandb.ai/sharonxin1207/uncategorized</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "text/html": [
              " View run at <a href='https://wandb.ai/sharonxin1207/uncategorized/runs/c3sx98g1' target=\"_blank\">https://wandb.ai/sharonxin1207/uncategorized/runs/c3sx98g1</a>"
            ],
            "text/plain": [
              "<IPython.core.display.HTML object>"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        },
        {
          "data": {
            "application/vnd.jupyter.widget-view+json": {
              "model_id": "01f1202f35d44b67996d2e392c91541f",
              "version_major": 2,
              "version_minor": 0
            },
            "text/plain": [
              "  0%|          | 0/119 [00:00<?, ?it/s]"
            ]
          },
          "metadata": {},
          "output_type": "display_data"
        }
      ],
      "source": [
        "os.environ['WANDB_CONSOLE'] = 'off'\n",
        "wandb.init()\n",
        "predictions = test(model_queen, test_loader)"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "id": "ZE1hRnvf0bFz"
      },
      "outputs": [],
      "source": [
        "### Create CSV file with predictions\n",
        "savepath = F\"/content/drive/MyDrive/submission.csv\"\n",
        "with open(\"./submission.csv\", \"w+\") as f:\n",
        "    f.write(\"id,label\\n\")\n",
        "    for i in range(len(predictions)):\n",
        "        f.write(\"{},{}\\n\".format(i, predictions[i]))"
      ]
    },
    {
      "cell_type": "code",
      "execution_count": null,
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "LjcammuCxMKN",
        "outputId": "617ec6a7-c31d-458d-860b-0f01d18b1df8"
      },
      "outputs": [
        {
          "name": "stdout",
          "output_type": "stream",
          "text": [
            "Warning: Looks like you're using an outdated API Version, please consider updating (server 1.5.12 / client 1.5.8)\n",
            "100% 19.3M/19.3M [00:00<00:00, 45.7MB/s]\n",
            "Successfully submitted to Frame-Level Speech Recognition"
          ]
        }
      ],
      "source": [
        "if SUBMIT_KAGGLE:\n",
        "  ### Submit to kaggle competition using kaggle API (Uncomment below to use)\n",
        "  !kaggle competitions submit -c 11-785-s23-hw1p2 -f ./submission.csv -m \"Test Submission\"\n",
        "\n",
        "  ### However, its always safer to download the csv file and then upload to kaggle"
      ]
    }
  ],
  "metadata": {
    "accelerator": "GPU",
    "colab": {
      "machine_shape": "hm",
      "provenance": []
    },
    "gpuClass": "standard",
    "kernelspec": {
      "display_name": "Python 3.9.10 64-bit",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "name": "python",
      "version": "3.9.10"
    },
    "vscode": {
      "interpreter": {
        "hash": "b0fa6594d8f4cbf19f97940f81e996739fb7646882a419484c72d19e05852a7e"
      }
    },
    "widgets": {
      "application/vnd.jupyter.widget-state+json": {
        "01a19f3cae8140f08100fd06c9bd5588": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "01f1202f35d44b67996d2e392c91541f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_523085f8915d46bc8f17cfd55a0a3403",
              "IPY_MODEL_8c33fa3a84534075ab96a9b316cf40b0",
              "IPY_MODEL_c6faf2a48ccf487aa6341fa92b8c932f"
            ],
            "layout": "IPY_MODEL_6837c6f58eeb437ca2679aa071c464ee"
          }
        },
        "03befaa3aa2247e08724aeab064069a9": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "30703581e2a44704b7706fcae6acf1a6": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "365817d30c914698a6e175273aaf8523": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "DescriptionStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "DescriptionStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "description_width": ""
          }
        },
        "3d491d5f793749fa81df3c61281a7c19": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_729642819a1446f8bfd4127c9f27baa5",
            "max": 1,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_7a0d1d45a0304b77a54c118dcd17c8d2",
            "value": 0.011160924224848733
          }
        },
        "4335326e40164eacad76938839f5b1d3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "4d9ef737bdf94013a8df17caa89618e9": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "523085f8915d46bc8f17cfd55a0a3403": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_a8b56afc621247db8d59731b02a740e3",
            "placeholder": "​",
            "style": "IPY_MODEL_03befaa3aa2247e08724aeab064069a9",
            "value": "100%"
          }
        },
        "5e79a744343740ad90a7590985f3f78d": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "6837c6f58eeb437ca2679aa071c464ee": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "729642819a1446f8bfd4127c9f27baa5": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "7a0d1d45a0304b77a54c118dcd17c8d2": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "894909de0f5d44b5b302001ae774be8a": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "VBoxModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "VBoxModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "VBoxView",
            "box_style": "",
            "children": [
              "IPY_MODEL_bb2bf458357e471ab97c07af5ebe0544",
              "IPY_MODEL_3d491d5f793749fa81df3c61281a7c19"
            ],
            "layout": "IPY_MODEL_5e79a744343740ad90a7590985f3f78d"
          }
        },
        "8c33fa3a84534075ab96a9b316cf40b0": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "FloatProgressModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "FloatProgressModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "ProgressView",
            "bar_style": "success",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4d9ef737bdf94013a8df17caa89618e9",
            "max": 119,
            "min": 0,
            "orientation": "horizontal",
            "style": "IPY_MODEL_ac5965e729624d238d053222a629c58f",
            "value": 119
          }
        },
        "a8b56afc621247db8d59731b02a740e3": {
          "model_module": "@jupyter-widgets/base",
          "model_module_version": "1.2.0",
          "model_name": "LayoutModel",
          "state": {
            "_model_module": "@jupyter-widgets/base",
            "_model_module_version": "1.2.0",
            "_model_name": "LayoutModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "LayoutView",
            "align_content": null,
            "align_items": null,
            "align_self": null,
            "border": null,
            "bottom": null,
            "display": null,
            "flex": null,
            "flex_flow": null,
            "grid_area": null,
            "grid_auto_columns": null,
            "grid_auto_flow": null,
            "grid_auto_rows": null,
            "grid_column": null,
            "grid_gap": null,
            "grid_row": null,
            "grid_template_areas": null,
            "grid_template_columns": null,
            "grid_template_rows": null,
            "height": null,
            "justify_content": null,
            "justify_items": null,
            "left": null,
            "margin": null,
            "max_height": null,
            "max_width": null,
            "min_height": null,
            "min_width": null,
            "object_fit": null,
            "object_position": null,
            "order": null,
            "overflow": null,
            "overflow_x": null,
            "overflow_y": null,
            "padding": null,
            "right": null,
            "top": null,
            "visibility": null,
            "width": null
          }
        },
        "ac5965e729624d238d053222a629c58f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "ProgressStyleModel",
          "state": {
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "ProgressStyleModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/base",
            "_view_module_version": "1.2.0",
            "_view_name": "StyleView",
            "bar_color": null,
            "description_width": ""
          }
        },
        "bb2bf458357e471ab97c07af5ebe0544": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "LabelModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "LabelModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "LabelView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_4335326e40164eacad76938839f5b1d3",
            "placeholder": "​",
            "style": "IPY_MODEL_365817d30c914698a6e175273aaf8523",
            "value": "0.002 MB of 0.183 MB uploaded (0.000 MB deduped)\r"
          }
        },
        "c6faf2a48ccf487aa6341fa92b8c932f": {
          "model_module": "@jupyter-widgets/controls",
          "model_module_version": "1.5.0",
          "model_name": "HTMLModel",
          "state": {
            "_dom_classes": [],
            "_model_module": "@jupyter-widgets/controls",
            "_model_module_version": "1.5.0",
            "_model_name": "HTMLModel",
            "_view_count": null,
            "_view_module": "@jupyter-widgets/controls",
            "_view_module_version": "1.5.0",
            "_view_name": "HTMLView",
            "description": "",
            "description_tooltip": null,
            "layout": "IPY_MODEL_01a19f3cae8140f08100fd06c9bd5588",
            "placeholder": "​",
            "style": "IPY_MODEL_30703581e2a44704b7706fcae6acf1a6",
            "value": " 119/119 [00:40&lt;00:00,  3.10it/s]"
          }
        }
      }
    }
  },
  "nbformat": 4,
  "nbformat_minor": 0
}
